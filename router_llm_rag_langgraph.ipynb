{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "#langgraph using llm and rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel,Field\n",
    "from langchain.output_parsers import PydanticOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "class outputparser(BaseModel):\n",
    "    topic:str=Field(description=\"selected topic\")\n",
    "    reasoning:str=Field(description=\"reasoning behind the topic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser=PydanticOutputParser(pydantic_object=outputparser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"topic\": {\"description\": \"selected topic\", \"title\": \"Topic\", \"type\": \"string\"}, \"reasoning\": {\"description\": \"reasoning behind the topic\", \"title\": \"Reasoning\", \"type\": \"string\"}}, \"required\": [\"topic\", \"reasoning\"]}\\n```'"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict,Annotated,Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import BaseMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages:Annotated[Sequence[BaseMessage],operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatGroq(model=\"gemma2-9b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(state):\n",
    "    print(\"--> ensure topic <--\")\n",
    "    messages=state[\"messages\"]\n",
    "    question=messages[-1]\n",
    "\n",
    "    template=\"\"\"\n",
    "        your task is to classify the given user query into one of the following category[langsmith,not Related]\n",
    "        only response with category and nothing else\n",
    "        user query:{question},\n",
    "        {format_instruction}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt=PromptTemplate(\n",
    "        input_variables=[question],\n",
    "        template=template,\n",
    "        format_instruction=parser.get_format_instructions()\n",
    "    )\n",
    "\n",
    "    chain=prompt|llm|StrOutputParser()\n",
    "\n",
    "    response=chain.invoke({\"question\":question,\"format_instruction\":parser.get_format_instructions()})\n",
    "\n",
    "    return {\"messages\":[response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "#state={\"messages\":[\"who is president of india\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function_1(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HF_TOKEN\"]=os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=PyPDFLoader(\"C:\\\\Users\\\\user\\\\OneDrive\\\\Desktop\\\\langchain\\\\temp.pdf\")\n",
    "docs=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "documets=RecursiveCharacterTextSplitter(chunk_size=400,chunk_overlap=50).split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'C:\\\\Users\\\\user\\\\OneDrive\\\\Desktop\\\\langchain\\\\temp.pdf', 'page': 0}, page_content='From Wikipedia, the free encyclopedia \\nAn illustration of main components of the \\ntransformer model from the paper \\n\"Attention Is All You Need\"[1] is a 2017 landmark[2][3] research paper in machine learning authored by \\neight scientists working at Google. The paper introduced a new deep learning architecture known as'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\user\\\\OneDrive\\\\Desktop\\\\langchain\\\\temp.pdf', 'page': 0}, page_content='the transformer, based on the attention mechanism proposed in 2014 by Bahdanau et al.[4] It is \\nconsidered a foundational[5] paper in modern artificial intelligence, as the transformer approach has \\nbecome the main architecture of large language models like those based on GPT.[6][7] At the time, the'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\user\\\\OneDrive\\\\Desktop\\\\langchain\\\\temp.pdf', 'page': 0}, page_content='focus of the research was on improving Seq2seq techniques for machine translation, but the authors \\ngo further in the paper, foreseeing the technique\\'s potential for other tasks like question \\nanswering and what is now known as multimodal Generative AI.[1] \\nThe paper\\'s title is a reference to the song \"All You Need Is Love\" by the Beatles.[8] The name'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\user\\\\OneDrive\\\\Desktop\\\\langchain\\\\temp.pdf', 'page': 0}, page_content='\"Transformer\" was picked because Uszkoreit liked the sound of that word.[9] \\nAn early design document was titled \"Transformers: Iterative Self-Attention and Processing for \\nVarious Tasks\", and included an illustration of six characters from the Transformers animated show. \\nThe team was named Team Transformer.[8]'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\user\\\\OneDrive\\\\Desktop\\\\langchain\\\\temp.pdf', 'page': 0}, page_content='The team was named Team Transformer.[8] \\nSome early examples that the team tried their Transformer architecture on included English-to-\\nGerman translation, generating Wikipedia articles on \"The Transformer\", and parsing. These \\nconvinced the team that the Transformer is a general purpose language model, and not just good for \\ntranslation.[9]'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\user\\\\OneDrive\\\\Desktop\\\\langchain\\\\temp.pdf', 'page': 0}, page_content='translation.[9] \\nAs of 2024, the paper has been cited more than 100,000 times.[10] \\nFor their 100M-parameter Transformer model, they suggested learning rate should be linearly scaled \\nup from 0 to maximal value for the first part of the training (i.e. 2% of the total number of training \\nsteps), and to use dropout, to stabilize training. \\nAuthors \\n[edit]'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\user\\\\OneDrive\\\\Desktop\\\\langchain\\\\temp.pdf', 'page': 0}, page_content='Authors \\n[edit] \\nThe authors of the paper are: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion \\nJones, Aidan Gomez, Lukasz Kaiser, and Illia Polosukhin. All eight authors were \"equal contributors\" \\nto the paper; the listed order was randomized. The Wired article highlights the group\\'s diversity:[8]'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\user\\\\OneDrive\\\\Desktop\\\\langchain\\\\temp.pdf', 'page': 1}, page_content='Six of the eight authors were born outside the United States; the other two are children of two \\ngreen-card-carrying Germans who were temporarily in California and a first-generation American \\nwhose family had fled persecution, respectively. \\nBy 2023, all eight authors had left Google and founded their own AI start-ups (except Łukasz Kaiser, \\nwho joined OpenAI).[8][10] \\nHistorical context \\n[edit]'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\user\\\\OneDrive\\\\Desktop\\\\langchain\\\\temp.pdf', 'page': 1}, page_content='Historical context \\n[edit] \\nMain articles: Transformer (deep learning architecture) § History, and Seq2seq § History \\nSee also: Timeline of machine learning \\nPredecessors \\n[edit] \\nFor many years, sequence modelling and generation was done by using plain recurrent neural \\nnetworks (RNNs). A well-cited early example was the Elman network (1990). In theory, the'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\user\\\\OneDrive\\\\Desktop\\\\langchain\\\\temp.pdf', 'page': 1}, page_content=\"information from one token can propagate arbitrarily far down the sequence, but in practice \\nthe vanishing-gradient problem leaves the model's state at the end of a long sentence without \\nprecise, extractable information about preceding tokens. \\nA key breakthrough was LSTM (1995),[note 1] a RNN which used various innovations to overcome the\"),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\user\\\\OneDrive\\\\Desktop\\\\langchain\\\\temp.pdf', 'page': 1}, page_content='vanishing gradient problem, allowing efficient learning of long-sequence modelling. One key \\ninnovation was the use of an attention mechanism which used neurons that multiply the outputs of \\nother neurons, so-called multiplicative units.[11] Neural networks using multiplicative units were later \\ncalled sigma-pi networks[12] or higher-order networks.[13] LSTM became the standard architecture for'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\user\\\\OneDrive\\\\Desktop\\\\langchain\\\\temp.pdf', 'page': 1}, page_content='long sequence modelling until the 2017 publication of Transformers. However, LSTM still used \\nsequential processing, like most other RNNs.[note 2] Specifically, RNNs operate one token at a time \\nfrom first to last; they cannot operate in parallel over all tokens in a sequence. \\nModern Transformers overcome this problem, but unlike RNNs, they require computation time that'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\user\\\\OneDrive\\\\Desktop\\\\langchain\\\\temp.pdf', 'page': 1}, page_content='is quadratic in the size of the context window. The linearly scaling fast weight controller (1992) learns \\nto compute a weight matrix for further processing depending on the input.[14] One of its two \\nnetworks has \"fast weights\" or \"dynamic links\" (1981).[15][16][17] A slow neural network learns by \\ngradient descent to generate keys and values for computing the weight changes of the fast neural'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\user\\\\OneDrive\\\\Desktop\\\\langchain\\\\temp.pdf', 'page': 1}, page_content='network which computes answers to queries.[14] This was later shown to be equivalent to the \\nunnormalized linear Transformer.[18][19] \\nAttention with seq2seq \\n[edit] \\nMain article: Seq2seq § History \\nThe idea of encoder-decoder sequence transduction had been developed in the early 2010s (see \\nprevious papers[20][21]). The papers most commonly cited as the originators that produced seq2seq'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\user\\\\OneDrive\\\\Desktop\\\\langchain\\\\temp.pdf', 'page': 1}, page_content='are two concurrently published papers from 2014.[20][21] \\nA 380M-parameter model for machine translation uses two long short-term memories (LSTM).[21] Its \\narchitecture consists of two parts. The encoder is an LSTM that takes in a sequence of tokens and'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\user\\\\OneDrive\\\\Desktop\\\\langchain\\\\temp.pdf', 'page': 2}, page_content='turns it into a vector. The decoder is another LSTM that converts the vector into a sequence of \\ntokens. Similarly, another 130M-parameter model used gated recurrent units (GRU) instead of \\nLSTM.[20] Later research showed that GRUs are neither better nor worse than LSTMs for \\nseq2seq.[22][23] \\nThese early seq2seq models had no attention mechanism, and the state vector is accessible only'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\user\\\\OneDrive\\\\Desktop\\\\langchain\\\\temp.pdf', 'page': 2}, page_content='after the last word of the source text was processed. Although in theory such a vector retains the \\ninformation about the whole original sentence, in practice the information is poorly preserved. This \\nis because the input is processed sequentially by one recurrent network into a fixed-size output \\nvector, which is then processed by another recurrent network into an output. If the input is long,'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\user\\\\OneDrive\\\\Desktop\\\\langchain\\\\temp.pdf', 'page': 2}, page_content='then the output vector would not be able to contain all relevant information, degrading the output. \\nAs evidence, reversing the input sentence improved seq2seq translation.[24] \\nThe RNNsearch model introduced an attention mechanism to seq2seq for machine translation to \\nsolve the bottleneck problem (of the fixed-size output vector), allowing the model to process long-'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\user\\\\OneDrive\\\\Desktop\\\\langchain\\\\temp.pdf', 'page': 2}, page_content='distance dependencies more easily. The name is because it \"emulates searching through a source \\nsentence during decoding a translation\".[4] \\nThe relative performances were compared between global (that of RNNsearch) and local (sliding \\nwindow) attention model architectures for machine translation, finding that mixed attention had'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\user\\\\OneDrive\\\\Desktop\\\\langchain\\\\temp.pdf', 'page': 2}, page_content='higher quality than global attention, while local attention reduced translation time.[25] \\nIn 2016, Google Translate was revamped to Google Neural Machine Translation, which replaced the \\nprevious model based on statistical machine translation. The new model was a seq2seq model where \\nthe encoder and the decoder were both 8 layers of bidirectional LSTM.[26] It took nine months to'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\user\\\\OneDrive\\\\Desktop\\\\langchain\\\\temp.pdf', 'page': 2}, page_content='develop, and it outperformed the statistical approach, which took ten years to develop.[27] \\nParallelizing attention \\n[edit] \\nMain article: Attention (machine learning) § History \\nSeq2seq models with attention (including self-attention) still suffered from the same issue with \\nrecurrent networks, which is that they are hard to parallelize, which prevented them to be'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\user\\\\OneDrive\\\\Desktop\\\\langchain\\\\temp.pdf', 'page': 2}, page_content='accelerated on GPUs. In 2016, decomposable attention applied a self-attention mechanism \\nto feedforward networks, which are easy to parallelize, and achieved SOTA result in textual \\nentailment with an order of magnitude less parameters than LSTMs.[28] One of its authors, Jakob \\nUszkoreit, suspected that attention without recurrence is sufficient for language translation, thus the'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\user\\\\OneDrive\\\\Desktop\\\\langchain\\\\temp.pdf', 'page': 2}, page_content='title \"attention is all you need\".[29] That hypothesis was against conventional wisdom of the time, and \\neven his father, a well-known computational linguist, was skeptical.[29] In the same year, self-attention \\n(called intra-attention or intra-sentence attention) was proposed for LSTMs.[30] \\nIn 2017, the original (100M-sized) encoder-decoder transformer model was proposed in the'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\user\\\\OneDrive\\\\Desktop\\\\langchain\\\\temp.pdf', 'page': 2}, page_content='\"Attention is all you need\" paper. At the time, the focus of the research was on \\nimproving seq2seq for machine translation, by removing its recurrence to process all tokens in \\nparallel, but preserving its dot-product attention mechanism to keep its text processing \\nperformance.[1] Its parallelizability was an important factor to its widespread use in large neural \\nnetworks.[31] \\nAI boom era'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\user\\\\OneDrive\\\\Desktop\\\\langchain\\\\temp.pdf', 'page': 2}, page_content='networks.[31] \\nAI boom era \\n[edit]'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\user\\\\OneDrive\\\\Desktop\\\\langchain\\\\temp.pdf', 'page': 3}, page_content='Already in spring 2017, even before the \"Attention is all you need\" preprint was published, one of the \\nco-authors applied the \"decoder-only\" variation of the architecture to generate fictitious Wikipedia \\narticles.[32] Transformer architecture is now used in many generative models that contribute to the \\nongoing AI boom.'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\user\\\\OneDrive\\\\Desktop\\\\langchain\\\\temp.pdf', 'page': 3}, page_content='ongoing AI boom. \\nIn language modelling, ELMo (2018) was a bi-directional LSTM that produces contextualized word \\nembeddings, improving upon the line of research from bag of words and word2vec. It was followed \\nby BERT (2018), an encoder-only Transformer model.[33] In 2019 October, Google started using BERT'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\user\\\\OneDrive\\\\Desktop\\\\langchain\\\\temp.pdf', 'page': 3}, page_content='to process search queries.[34] In 2020, Google Translate replaced the previous RNN-encoder–RNN-\\ndecoder model by a Transformer-encoder–RNN-decoder model.[35] \\nStarting in 2018, the OpenAI GPT series of decoder-only Transformers became state of the art \\nin natural language generation. In 2022, a chatbot based on GPT-3, ChatGPT, became unexpectedly'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\user\\\\OneDrive\\\\Desktop\\\\langchain\\\\temp.pdf', 'page': 3}, page_content='popular,[36] triggering a boom around large language models.[37][38] \\nSince 2020, Transformers have been applied in modalities beyond text, including the vision \\ntransformer,[39] speech recognition,[40] robotics,[41] and multimodal.[42] The vision transformer, in turn, \\nstimulated new developments in convolutional neural networks.[43] Image and video generators'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\user\\\\OneDrive\\\\Desktop\\\\langchain\\\\temp.pdf', 'page': 3}, page_content='like DALL-E (2021), Stable Diffusion 3 (2024),[44] and Sora (2024), are based on the Transformer \\narchitecture.')]"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "db=FAISS.from_documents(embedding=embeddings,documents=documets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000207895B52B0>, search_kwargs={})"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "def router(state):\n",
    "    print(\"->calling route <-\")\n",
    "\n",
    "    messages=state[\"messages\"]\n",
    "    last_message=messages[-1]\n",
    "    print(last_message)\n",
    "    if \"langsmith\" in last_message:\n",
    "        return \"RAG Call\"\n",
    "    else:\n",
    "        return \"LLM Call\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(state):\n",
    "    print(\"-->calling rag <--\")\n",
    "    messages=state[\"messages\"]\n",
    "    question=messages[0]\n",
    "    #print(question)\n",
    "    template=\"\"\"Answer the following question based on the given context\n",
    "    {context}\n",
    "    Question:{question}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt=ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    retrieval=prompt|llm|StrOutputParser()\n",
    "\n",
    "    response=retrieval.invoke({\"context\":retriever,\"question\":question})\n",
    "\n",
    "    return {\"messages\":[response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_3(state):\n",
    "    print(\"-->calling llm <--\")\n",
    "    messages=state[\"messages\"]\n",
    "    question=messages[0]\n",
    "    response=llm.invoke(question)\n",
    "    return {\"messages\":[response.content]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph,END,START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow=StateGraph(AgentState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x207822c22a0>"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.add_node(\"agent\",function_1)\n",
    "\n",
    "workflow.add_node(\"RAG\",function_2)\n",
    "\n",
    "workflow.add_node(\"LLM\",function_3)\n",
    "\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "\n",
    "    router,\n",
    "    {\n",
    "        \"RAG Call\":\"RAG\",\n",
    "        \"LLM Call\":\"LLM\",\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"RAG\",END)\n",
    "workflow.add_edge(\"LLM\",END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph=workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display,Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "input={\"messages\":[\"give some motivation for college students\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> ensure topic <--\n",
      "->calling route <-\n",
      "{\"topic\": \"not Related\", \"reasoning\": \"The query is seeking general motivation, not related to language modeling\"} \n",
      "\n",
      "-->calling llm <--\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': ['give some motivation for college students',\n",
       "  '{\"topic\": \"not Related\", \"reasoning\": \"The query is seeking general motivation, not related to language modeling\"} \\n',\n",
       "  '##  Fueling Your College Fire: Motivation for the Journey\\n\\nCollege can be a whirlwind of excitement, challenges, and self-discovery.  Here\\'s some fuel to keep your fire burning bright:\\n\\n**Remember Your \"Why\":**\\n\\n* **Passion:** What ignited your desire for higher education?  Reconnect with that spark, whether it\\'s a specific field, a thirst for knowledge, or a dream you\\'re chasing.\\n* **Impact:**  How will your education empower you to make a difference in the world?  Visualize the positive impact you can have on your community, your career, or even the lives of others.\\n* **Growth:** College isn\\'t just about grades; it\\'s about personal and intellectual growth. Embrace the opportunities to learn new things, challenge your beliefs, and expand your horizons.\\n\\n**Navigate the Challenges:**\\n\\n* **Embrace the Struggle:**  Learning is rarely easy.  View challenges as stepping stones to success, opportunities to learn and grow stronger.  \\n* **Seek Support:** Don\\'t be afraid to ask for help. Professors, advisors, classmates, and support services are there to guide you.\\n* **Celebrate Small Wins:**  Acknowledge your progress, no matter how small. Each milestone, each hurdle overcome, is a victory worth celebrating.\\n\\n**Fuel Your Journey:**\\n\\n* **Connect with Others:**  Build meaningful relationships with classmates, professors, and mentors.  Collaboration and community can make all the difference.\\n* **Explore Your Interests:**  Take advantage of clubs, organizations, and extracurricular activities to discover new passions and make lasting memories.\\n* **Prioritize Self-Care:**  Take care of your physical and mental health.  Get enough sleep, eat well, exercise, and make time for activities that bring you joy.\\n\\n**Remember:**\\n\\n* **You are capable of amazing things.**\\n* **Your journey is unique and valuable.**\\n* **Never stop believing in yourself.**\\n\\n\\nCollege is a transformative experience. Embrace the challenges, celebrate the victories, and most importantly, enjoy the ride!\\n']}"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke(input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
